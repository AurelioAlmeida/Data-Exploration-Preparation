path <- "/Users/renner/Documents/BostonHousing.csv"
library(mlbench)
Dataset <- read.csv(path, sep = ",", dec = ".", row.names = 1)
str(Dataset)
dim(Dataset)
Dataset = Dataset[-4]
dim(Dataset)
Scores <- linear.correlation(medv~., Dataset)
install.packages("rJava")
Scores <- linear.correlation(medv~., Dataset)
Scores
subset <- cutoff.k(Scores, 5)
subse2 <- cutoff.k.percent(Scores, 5)
View(Dataset)
library(dplyr)
library(tidyr)
library(ggplot2)
library(grid)
library(zoo)
library(FSelector)
install.packages("rJava")
install.packages("FSelector")
install.packages("mlbench")
install.packages("FSelector")
library(FSelector)
path <- "/Users/renner/Documents/BostonHousing.csv"
library(mlbench)
Dataset <- read.csv(path, sep = ",", dec = ".", row.names = 1)
str(Dataset)
dim(Dataset)
Dataset = Dataset[-4]
dim(Dataset)
Scores <- linear.correlation(medv~., Dataset)
Scores
subset <- cutoff.k(Scores, 5)
print(Subset)
Scores <- linear.correlation(medv~., Dataset)
Scores
subset <- cutoff.k(Scores, 5)
print(Subset)
Subset2 <-cutoff.k.percent(Scores, 0.6)
as.data.frame(Subset2)
print(Subset2)
print(as.data.frame(Subset2))
head(Dataset)
Scores2 <- information.gain(medv~., Dataset)
head(Scores2, 13)
Subset3 <- cutoff.k(Scores2, 5)
as.data.frame(Subset3)
install.packages("mclust")
install.packages("cluster")
install.packages("clustvarsel")
install.packages("wskm")
library(cluster)
library(clustvarsel)
library(mclust)
library(wskm)
set.seed(2)
model <- ewkm(iris[1:4], 3, lambda=2, maxiter=1000)
clusplot(iris[1:4], model$cluster, color = TRUE, shade = TRUE,
labels = 2, lines = 1, main = 'Cluster Analysis for Iris')
round(model$weights*100,2)
path<-"/Users/renner/Documents/syntheticdata.csv"
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
head(Dataset)
out = clustvarsel(Dataset, G = 1:5)
out
out$subset
Subset1 = Dataset[,out$subset]
mod = Mclust(Subset1, G = 1:5)
summary(mod)
plot(mod,c("classification"))
path<-"/Users/renner/Documents/BostonHousing.csv"
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
Dataset<-Dataset[-4]
head(Dataset,3)
install.packages("caret")
library(caret)
install.packages("corrplot")
library(corrplot)
correlationMatrix <- cor(Dataset)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated
names(Dataset[,highlyCorrelated])
Dataset2<-Dataset[-highlyCorrelated]
par(mfrow = c(1, 2))
corrplot(correlationMatrix, order = "hclust")
corrplot(cor(Dataset2), order = "hclust")
Scores <- linear.correlation(medv~., Dataset)
Scores
subset <- cutoff.k(Scores, 5)
print(subset)
Subset2 <-cutoff.k.percent(Scores, 0.6)
as.data.frame(Subset2)
print(Subset2)
print(as.data.frame(Subset2))
head(Dataset)
Scores2 <- information.gain(medv~., Dataset)
Subset3 <- cutoff.k(Scores2, 5)
as.data.frame(Subset3)
install.packages("mclust")
install.packages("mclust")
install.packages("cluster")
install.packages("clustvarsel")
install.packages("wskm")
library(cluster)
library(clustvarsel)
library(mclust)
library(wskm)
set.seed(2)
model <- ewkm(iris[1:4], 3, lambda=2, maxiter=1000)
clusplot(iris[1:4], model$cluster, color = TRUE, shade = TRUE,
labels = 2, lines = 1, main = 'Cluster Analysis for Iris')
round(model$weights*100,2)
path<-"/Users/renner/Documents/syntheticdata.csv" # Set your Path Here
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
head(Dataset)
out = clustvarsel(Dataset, G = 1:5)
out
out$subset
Subset1 = Dataset[,out$subset]
mod = Mclust(Subset1, G = 1:5)
summary(mod)
plot(mod,c("classification"))
path<-"/Users/renner/Documents/BostonHousing.csv"
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
Dataset<-Dataset[-4]
head(Dataset,3)
install.packages("caret")
library(caret)
install.packages("corrplot")
library(corrplot)
correlationMatrix <- cor(Dataset)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated
names(Dataset[,highlyCorrelated])
Dataset2<-Dataset[-highlyCorrelated]
par(mfrow = c(1, 2))
corrplot(correlationMatrix, order = "hclust")
corrplot(cor(Dataset2), order = "hclust")
View(Dataset)
View(Dataset2)
clusplot(iris[1:4], model$cluster, color = TRUE, shade = TRUE,
labels = 2, lines = 1, main = 'Cluster Analysis for Iris')
round(model$weights*100,2)
path<-"/Users/renner/Documents/syntheticdata.csv" # Set your Path Here
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
head(Dataset)
out = clustvarsel(Dataset, G = 1:5)
out
out$subset
Subset1 = Dataset[,out$subset]
mod = Mclust(Subset1, G = 1:5)
summary(mod)
plot(mod,c("classification"))
getwd()
clusplot(iris[1:4], model$cluster, color = TRUE, shade = TRUE,
labels = 2, lines = 1, main = 'Cluster Analysis for Iris')
round(model$weights*100,2)
path<-"/Users/renner/Documents/syntheticdata.csv"
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
head(Dataset)
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
head(Dataset)
out = clustvarsel(Dataset, G = 1:5)
out
out$subset
Subset1 = Dataset[,out$subset]
mod = Mclust(Subset1, G = 1:5)
summary(mod)
plot(mod,c("classification"))
path<-"/Users/renner/Documents/BostonHousing.csv"
Dataset<-read.csv(path, sep = ",", dec = ".",row.names = 1)
Dataset<-Dataset[-4]
head(Dataset,3)
install.packages("caret")
library(caret)
install.packages("caret")
install.packages("corrplot")
library(corrplot)
correlationMatrix <- cor(Dataset)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated
names(Dataset[,highlyCorrelated])
Dataset2<-Dataset[-highlyCorrelated]
par(mfrow = c(1, 2))
corrplot(correlationMatrix, order = "hclust")
corrplot(cor(Dataset2), order = "hclust")
install.packages(c("bslib", "cluster", "cpp11", "dbplyr", "deldir", "diptest", "dplyr", "evaluate", "foreign", "ggplot2", "haven", "htmltools", "interp", "knitr", "lattice", "lava", "lifecycle", "Matrix", "mclust", "mvtnorm", "nlme", "pROC", "RcppEigen", "rJava", "rlang", "robustbase", "scales", "stringi", "stringr", "tinytex", "utf8", "vctrs", "withr", "xfun"))
install.packages(c("jsonlite", "xml2"))
install.packages(c("e1071", "rpart", "sass", "vroom"))
plt.plot(eu_dataset['date'], eu_dataset['value'])
plt.plot(eu_dataset['date'], eu_dataset['value'])
plot(eu_dataset$date, eu_dataset$new_cases, type='l', col='blue', xlab='Date', ylab='New Cases', main='Line Plot of New Cases Over Time')
plot(eu_dataset$date, eu_dataset$new_cases, type='l', col='blue', xlab='Date', ylab='New Cases', main='Line Plot of New Cases Over Time')
plot(eu_dataset$date, eu_dataset$new_cases, type='l', col='blue', xlab='Date', ylab='New Cases', main='Line Plot of New Cases Over Time')
eu_dataset$date <- as.Date(eu_dataset$date)
plot(eu_dataset$date, eu_dataset$new_cases, type='l', col='blue', xlab='Date', ylab='New Cases', main='Line Plot of New Cases Over Time')
install.packages(c("e1071", "progress", "sass", "tseries"))
plot(eu_dataset$date, eu_dataset$value, type='l', col='blue', xlab='Date', ylab='Value', main='Line Plot of Value Over Time')
Global_Covid_Dataset_ <- read.csv("C:/Users/AurelioAlmeida/Documents/GitHub/Data-Exploration-Preparation/Global Covid Dataset .csv")
Global_Covid_Dataset_ <- read.csv("C:/Users/AurelioAlmeida/Documents/GitHub/Data-Exploration-Preparation/Global Covid Dataset .csv")
Global_Covid_Dataset_ <- read.csv("C:/Users/AurelioAlmeida/Documents/GitHub/Data-Exploration-Preparation/Global Covid Dataset .csv")
setwd("~/Documents/GitHub/Data-Exploration-Preparation")
library(readr)
Global_Covid_Dataset_ <- read_csv("Global Covid Dataset .csv")
View(Global_Covid_Dataset_)
# List of EU countries
eu_countries <- c(
"Austria", "Belgium", "Bulgaria", "Croatia", "Cyprus", "Czech Republic",
"Denmark", "Estonia", "Finland", "France", "Germany", "Greece",
"Hungary", "Ireland", "Italy", "Latvia", "Lithuania", "Luxembourg",
"Malta", "Netherlands", "Poland", "Portugal", "Romania", "Slovakia",
"Slovenia", "Spain", "Sweden"
)
# Filter the dataset for EU countries
eu_dataset <- subset(Global_Covid_Dataset_, location %in% eu_countries)
# Adjust axis limits
ggplot(subset(eu_dataset, location %in% top_countries), aes(x = date, y = total_cases, color = location)) +
geom_point() +
labs(title = 'Total Cases Over Time - Top 5 Countries',
x = 'Date',
y = 'Total Cases (in Millions)') +
theme_minimal() +
scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "M")) +  # Format labels in millions
scale_x_date(limits = c(min(eu_dataset$date), max(eu_dataset$date)))  # Adjust x-axis limits
# Assuming you have already loaded your dataset into a data frame named eu_dataset
# If not, you can load it using: eu_dataset <- read.csv('your_dataset.csv')
# Install and load the necessary library if not already installed
if (!require(ggplot2)) {
install.packages("ggplot2")
}
library(ggplot2)
# Select the top 5 countries with the highest total cases
top_countries <- names(sort(tapply(eu_dataset$total_cases, eu_dataset$location, max), decreasing = TRUE)[1:5])
# Convert date to Date type if it's not already
eu_dataset$date <- as.Date(eu_dataset$date)
# Plot scatter plots for each of the top 5 countries with numeric format for total cases
ggplot(subset(eu_dataset, location %in% top_countries), aes(x = date, y = total_cases, color = location)) +
geom_point() +
labs(title = 'Total Cases Over Time - Top 5 Countries',
x = 'Date',
y = 'Total Cases (in Millions)') +
theme_minimal() +
scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "M"))  # Format labels in millions
library(readr)
library(visdat)
library(caret)
library(zoo)
library(imputeTS)
library(ggplot2)
# Select the top 5 countries with the highest total cases
top_countries <- names(sort(tapply(eu_dataset$total_cases, eu_dataset$location, max), decreasing = TRUE)[1:5])
# Convert date to Date type if it's not already
eu_dataset$date <- as.Date(eu_dataset$date)
# Plot scatter plots for each of the top 5 countries with numeric format for total cases
ggplot(subset(eu_dataset, location %in% top_countries), aes(x = date, y = total_cases, color = location)) +
geom_point() +
labs(title = 'Total Cases Over Time - Top 5 Countries',
x = 'Date',
y = 'Total Cases (in Millions)') +
theme_minimal() +
scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "M"))  # Format labels in millions
View(Dataset)
View(eu_dataset)
# Check dataset structure
str(data)
# Display dataset first few rows
head(data)
# Check number of rows
num_rows <- nrow(data)
print(num_rows)
# List of EU countries
eu_countries <- c(
"Austria", "Belgium", "Bulgaria", "Croatia", "Cyprus", "Czech Republic",
"Denmark", "Estonia", "Finland", "France", "Germany", "Greece",
"Hungary", "Ireland", "Italy", "Latvia", "Lithuania", "Luxembourg",
"Malta", "Netherlands", "Poland", "Portugal", "Romania", "Slovakia",
"Slovenia", "Spain", "Sweden"
)
# Filter the dataset for EU countries
eu_dataset <- subset(Global_Covid_Dataset_, location %in% eu_countries)
# Check data types
str(eu_dataset)
# Get summary statistics
summary(eu_dataset)
# Visualize missing data
vis_dat(eu_dataset)
# Calculate statistical parameters
summary_stats <- summary(eu_dataset[, c("new_cases", "new_deaths", "total_cases", "total_deaths", "weekly_cases", "weekly_deaths", "biweekly_cases", "biweekly_deaths")])
# Extract mean, median, min, max, and sd for new_cases
mean_new_cases <- mean(eu_dataset$new_cases, na.rm = TRUE)
median_new_cases <- median(eu_dataset$new_cases, na.rm = TRUE)
min_new_cases <- min(eu_dataset$new_cases, na.rm = TRUE)
max_new_cases <- max(eu_dataset$new_cases, na.rm = TRUE)
sd_new_cases <- sd(eu_dataset$new_cases, na.rm = TRUE)
# Print or use these values
print(mean_new_cases)
print(median_new_cases)
print(min_new_cases)
print(max_new_cases)
print(sd_new_cases)
# Calculate summary statistics for new_deaths
summary_new_deaths <- summary(eu_dataset$new_deaths)
# Extract values
mean_new_deaths <- mean(eu_dataset$new_deaths, na.rm = TRUE)
median_new_deaths <- median(eu_dataset$new_deaths, na.rm = TRUE)
min_new_deaths <- min(eu_dataset$new_deaths, na.rm = TRUE)
max_new_deaths <- max(eu_dataset$new_deaths, na.rm = TRUE)
sd_new_deaths <- sd(eu_dataset$new_deaths, na.rm = TRUE)
# Print values for new_deaths
print(mean_new_deaths)
print(median_new_deaths)
print(min_new_deaths)
print(max_new_deaths)
print(sd_new_deaths)
# Calculate summary statistics for total_cases
summary_total_cases <- summary(eu_dataset$total_cases)
# Extract values
mean_total_cases <- mean(eu_dataset$total_cases, na.rm = TRUE)
median_total_cases <- median(eu_dataset$total_cases, na.rm = TRUE)
min_total_cases <- min(eu_dataset$total_cases, na.rm = TRUE)
max_total_cases <- max(eu_dataset$total_cases, na.rm = TRUE)
sd_total_cases <- sd(eu_dataset$total_cases, na.rm = TRUE)
# Print values for total_cases
print(mean_total_cases)
print(median_total_cases)
print(min_total_cases)
print(max_total_cases)
print(sd_total_cases)
# Calculate summary statistics for total_deaths
summary_total_deaths <- summary(eu_dataset$total_deaths)
# Extract values
mean_total_deaths <- mean(eu_dataset$total_deaths, na.rm = TRUE)
median_total_deaths <- median(eu_dataset$total_deaths, na.rm = TRUE)
min_total_deaths <- min(eu_dataset$total_deaths, na.rm = TRUE)
max_total_deaths <- max(eu_dataset$total_deaths, na.rm = TRUE)
sd_total_deaths <- sd(eu_dataset$total_deaths, na.rm = TRUE)
# Print values for total_deaths
print(mean_total_deaths)
print(median_total_deaths)
print(min_total_deaths)
print(max_total_deaths)
print(sd_total_deaths)
# Calculate summary statistics for weekly_cases
summary_weekly_cases <- summary(eu_dataset$weekly_cases)
# Extract values
mean_weekly_cases <- mean(eu_dataset$weekly_cases, na.rm = TRUE)
median_weekly_cases <- median(eu_dataset$weekly_cases, na.rm = TRUE)
min_weekly_cases <- min(eu_dataset$weekly_cases, na.rm = TRUE)
max_weekly_cases <- max(eu_dataset$weekly_cases, na.rm = TRUE)
sd_weekly_cases <- sd(eu_dataset$weekly_cases, na.rm = TRUE)
# Print values for weekly_cases
print(mean_weekly_cases)
print(median_weekly_cases)
print(min_weekly_cases)
print(max_weekly_cases)
print(sd_weekly_cases)
# Calculate summary statistics for weekly_cases
summary_weekly_cases <- summary(eu_dataset$weekly_cases)
# Extract values
mean_weekly_cases <- mean(eu_dataset$weekly_cases, na.rm = TRUE)
median_weekly_cases <- median(eu_dataset$weekly_cases, na.rm = TRUE)
min_weekly_cases <- min(eu_dataset$weekly_cases, na.rm = TRUE)
max_weekly_cases <- max(eu_dataset$weekly_cases, na.rm = TRUE)
sd_weekly_cases <- sd(eu_dataset$weekly_cases, na.rm = TRUE)
# Print values for weekly_cases
print(mean_weekly_cases)
print(median_weekly_cases)
print(min_weekly_cases)
print(max_weekly_cases)
print(sd_weekly_cases)
# Calculate summary statistics for weekly_deaths
summary_weekly_deaths <- summary(eu_dataset$weekly_deaths)
# Extract values
mean_weekly_deaths <- mean(eu_dataset$weekly_deaths, na.rm = TRUE)
median_weekly_deaths <- median(eu_dataset$weekly_deaths, na.rm = TRUE)
min_weekly_deaths <- min(eu_dataset$weekly_deaths, na.rm = TRUE)
max_weekly_deaths <- max(eu_dataset$weekly_deaths, na.rm = TRUE)
sd_weekly_deaths <- sd(eu_dataset$weekly_deaths, na.rm = TRUE)
# Print values for weekly_deaths
print(mean_weekly_deaths)
print(median_weekly_deaths)
print(min_weekly_deaths)
print(max_weekly_deaths)
print(sd_weekly_deaths)
# Calculate summary statistics for biweekly_cases
summary_biweekly_cases <- summary(eu_dataset$biweekly_cases)
# Extract values
mean_biweekly_cases <- mean(eu_dataset$biweekly_cases, na.rm = TRUE)
median_biweekly_cases <- median(eu_dataset$biweekly_cases, na.rm = TRUE)
min_biweekly_cases <- min(eu_dataset$biweekly_cases, na.rm = TRUE)
max_biweekly_cases <- max(eu_dataset$biweekly_cases, na.rm = TRUE)
sd_biweekly_cases <- sd(eu_dataset$biweekly_cases, na.rm = TRUE)
# Print values for biweekly_cases
print(mean_biweekly_cases)
print(median_biweekly_cases)
print(min_biweekly_cases)
print(max_biweekly_cases)
print(sd_biweekly_cases)
# Calculate summary statistics for biweekly_deaths
summary_biweekly_deaths <- summary(eu_dataset$biweekly_deaths)
# Extract values
mean_biweekly_deaths <- mean(eu_dataset$biweekly_deaths, na.rm = TRUE)
median_biweekly_deaths <- median(eu_dataset$biweekly_deaths, na.rm = TRUE)
min_biweekly_deaths <- min(eu_dataset$biweekly_deaths, na.rm = TRUE)
max_biweekly_deaths <- max(eu_dataset$biweekly_deaths, na.rm = TRUE)
sd_biweekly_deaths <- sd(eu_dataset$biweekly_deaths, na.rm = TRUE)
# Print values for biweekly_deaths
print(mean_biweekly_deaths)
print(median_biweekly_deaths)
print(min_biweekly_deaths)
print(max_biweekly_deaths)
print(sd_biweekly_deaths)
###start of letter C
##Handling missing values before perform scaling methods
# Interpolate missing values of "new_cases" variable
eu_dataset$new_cases <- na.approx(eu_dataset$new_cases, na.rm = FALSE)
##Min-Max Normalization
# Extract the variable
new_cases <- eu_dataset$new_cases
# Function to apply Min-Max Normalization
min_max_normalize <- function(x) {
return((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
# Apply Min-Max Normalization
new_cases_min_max <- min_max_normalize(new_cases)
# Min-Max Normalization
summary_min_max_new_cases <- summary(new_cases_min_max)
# Print values
cat("\nSummary Statistics - Min-Max Normalization:\n")
cat(paste(names(summary_min_max_new_cases), ": ", summary_min_max_new_cases, sep="\n"), "\n")
## Z-score Standardization
# Interpolate missing values using linear interpolation
eu_dataset$new_cases <- na.interpolation(eu_dataset$new_cases, option = "linear")
# Function to apply Z-score Standardization
z_score_standardize_new_cases <- function(x) {
return((x - mean(x)) / sd(x))
}
# Extract the variable
new_cases <- eu_dataset$new_cases
# Apply Z-score Standardization
new_cases_z_score <- z_score_standardize_new_cases(new_cases)
# Summary Statistics - Z-score Standardization
summary_z_score_new_cases <- summary(new_cases_z_score)
# Print values
cat("\nSummary Statistics - Z-score Standardization (new_cases):\n")
cat(paste(names(summary_z_score_new_cases), ": ", summary_z_score_new_cases, sep="\n"), "\n")
##Robust scalar
# Interpolate missing values of "new_cases" variable
eu_dataset$new_cases <- na.approx(eu_dataset$new_cases, na.rm = FALSE)
# Extract the variable
new_cases <- eu_dataset$new_cases
# Function to apply Robust Scaling
robust_scale <- function(x) {
return((x - median(x)) / IQR(x))
}
# Apply Robust Scaling
new_cases_robust <- robust_scale(new_cases)
# Summary Statistics - Robust Scaling
summary_robust <- summary(new_cases_robust)
# Print values
cat("\nSummary Statistics - Robust Scaling (new_cases):\n")
cat(paste(names(summary_robust), ": ", summary_robust, sep="\n"), "\n")
# Select the top 5 countries with the highest total cases
top_countries <- names(sort(tapply(eu_dataset$total_cases, eu_dataset$location, max), decreasing = TRUE)[1:5])
# Convert date to Date type if it's not already
eu_dataset$date <- as.Date(eu_dataset$date)
# Plot scatter plots for each of the top 5 countries with numeric format for total cases
ggplot(subset(eu_dataset, location %in% top_countries), aes(x = date, y = total_cases, color = location)) +
geom_point() +
labs(title = 'Total Cases Over Time - Top 5 Countries',
x = 'Date',
y = 'Total Cases (in Millions)') +
theme_minimal() +
scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "M"))  # Format labels in millions
# Select the top 5 countries with the highest total cases
top_countries <- names(sort(tapply(eu_dataset$total_cases, eu_dataset$location, max), decreasing = TRUE)[1:5])
# Convert date to Date type if it's not already
eu_dataset$date <- as.Date(eu_dataset$date)
# Plot scatter plots for each of the top 5 countries with numeric format for total cases
ggplot(subset(eu_dataset, location %in% top_countries), aes(x = date, y = total_cases, color = location)) +
geom_point() +
labs(title = 'Total Cases Over Time - Top 5 Countries',
x = 'Date',
y = 'Total Cases (in Millions)') +
theme_minimal() +
scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "M"))  # Format labels in millions
